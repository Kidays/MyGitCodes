## Some Notations
- $\theta_t$:model parameters at time step t
- $\nabla(\theta_t)$ or $g_t$:gradient at $\theta_t$,used to compute $\theta_{t+1}$
- $m_{t+1}$:momentum accumulated from time step 0 to time step $t$,which is used to compute $\theta_{t+1}$
## What is Optimization about?
- Find a $\theta$ to get the lowest $\sum_{x}L(\theta;x)$
- Or,Find a $\theta$ to get the lowest $L(\theta)$
## On-line vs Off-line
- On-line:one pair of $(x_t,\hat{y}_t)$ at a time step
- Off-line:pour all $(x_t,\hat{y}_t)$ into the model at every time step
- The rest of this lecture will focus on the off-line cases
## SGD
  start at position $\theta^0\\$
  Compute gradient at $\theta^0\\$
  Move to $\theta^1=\theta^0-\eta\nabla L(\theta^0)\\$
  Compute gradient at $\theta^1$
  Move to $\theta^2=\theta^1-\eta\nabla L(\theta^1)\\$
  ...
  Stop until $\nabla L(\theta^t)\approx 0$
## SGD with Momentum(SGDM)
- Movement not just based on gradient,but previous movement
- 
## ADAM
- Adam:fast training,large generalization gap,unstable
- SGDM:stable,little generalization gap,better convergence