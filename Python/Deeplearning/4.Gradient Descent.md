## Tuning your learning rate
- Set the learning rate $\eta$ carefully
## Adaptive Learning Rates
- Popular & Simple Idea:Reduce the learning rate by some factor every few epochs
  - At the beginning,we are far from the destination,so we use larger learning rate
  - After several epochs,we are close to the destination,so we reduce the learning rate
  - E.g.1/t decay: $\eta ^t=\eta \sqrt{t+1}$
- Learning rate cannot be one-size-fits-all
  - Giving different parameters different learning rates
## Adagrad
- Divide the learning rate of each parameter by the root mean square of its previous derivatives
  - **Vanilla Gradient descent**
$w^{t+1}\leftarrow w^t-\eta^tg^t$
    - w is oneparameters

  - **Adagrad**
$w^{t+1}\leftarrow w^t-\frac{\eta ^t}{\sigma ^t}g^t$
    - $\sigma ^t$: root mean square of the previous derivatives of parameter w
    - $\sigma ^t=\sqrt{\frac{1}{t+1}\sum_{i=0}^{t}(g^i)^2}$
    - $w^{t+1}\leftarrow w^t-\frac{\eta}{\sqrt{\sum_{i=0}^{t}(g^i)^2}}g^t$
  - Second Derivative
    - $\begin{vmatrix}
  \frac{\partial y}{\partial x}\\
  \end{vmatrix}=|2ax+b|$
    - $\frac{\partial^2 y}{\partial x^2}=2a$
    - The best step is $\frac{|First \, derivative|}{Second \,derivative}$
## Stochastic Gradient Descent
- Gradient Descent:
$L=\sum _{n}(\hat{y}^n-(b+\sum w_ix_i^n))^2$
$\theta ^i=\theta ^{i-1}-\eta \nabla L(\theta^{i-1})$
- Stochastic Gradient Descent
  Pick an example $x^n$
  $L^n=(\hat{y}^n-(b+\sum w_ix_i^n))^2$
  $\theta ^i=\theta^{i-1}-\eta\nabla L^n(\theta^{i-1})$
## Feature Scaling
$y=b+w_1x_1+w_2x_2$
## Taylor Series
- **Taylor series**
  Let $h(x)$ be any function infinitely differentiable around $x=x_0$
$\\h(x)=\sum_{k=0}^{\infty}\frac{h^{(k)}(x_0)}{k!}(x-x_0)^k\\$ 
  when x is close to $x_0$:
$\\h(x)\approx h(x_0)+h'(x_0)(x-x_0)$
- Multivariable Taylor Series
  $\\h(x,y)=h(x_0)(y_0)+\frac{\partial h(x_0,y_0)}{\partial x}(x-x_0)+\frac{\partial h(x_0,y_0)}{\partial y}(y-y_0)$+something related to $(x-x_0)^2$ and $(y-y_0)^2\\$
  when $x$ and $y$ is close to $x_0$ and $y_0\\$
  $h(x,y)\approx h(x_0)(y_0)+\frac{\partial h(x_0,y_0)}{\partial x}(x-x_0)+\frac{\partial h(x_0,y_0)}{\partial y}(y-y_0)$
- Back to Formal Derivation
  Based on Taylor Series:
  $\\L(\theta)\approx L(a,b)+\frac{\partial L(a,b)}{\partial \theta_1}(\theta _1-a)+\frac{\partial L(a,b)}{\partial\theta_2}(\theta_2-b)$
  $\\s=L(a,b)$
  $\\u=\frac{\partial L(a,b)}{\partial \theta_1}$
  $\\v=\frac{\partial L(a,b)}{\partial \theta_2}$
  $\\L(\theta)\approx x+u(\theta_1-a)+v(\theta_2-b)\\$ d is **small enough**
  Find $\theta_1$ and $\theta_2$ in the circle minimizing $L(\theta)$
  $\\\frac{(\theta_1-a)^2}{\Delta \theta_1}+\frac{(\theta_1-b)^2}{\Delta \theta_2}\leq d^2\\$
  To minimize $L(\theta)\\$
  $\begin{bmatrix}
  \Delta\theta_1\\
  \Delta\theta_2\\
  \end{bmatrix}=-\eta\begin{bmatrix}
  u\\
  v\\
  \end{bmatrix}\\$
  $\begin{bmatrix}
  \theta_1\\
  \theta_2\\
  \end{bmatrix}=\begin{bmatrix}
  a\\
  b\\
  \end{bmatrix}-\eta\begin{bmatrix}
  u\\
  v\\
  \end{bmatrix}\\$
##More Limitation of Gradient Descent
- $\partial L/\partial w=0$
- $\partial L/\partial w\approx 0$

